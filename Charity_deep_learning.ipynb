{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabroo101/deep-learning-challenge/blob/main/Charity_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lMMRIx2P_l"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsD3PGba2P_p"
      },
      "source": [
        "---\n",
        "\n",
        "# **Charity Deep Learning Project**\n",
        "\n",
        "In this project, we'll be exploring deep learning techniques to understand and make predictions related to charity data.\n",
        "\n",
        "## **Step 1: Importing Dependencies**\n",
        "\n",
        "Before we dive into the actual data analysis, let's import all the necessary libraries and modules:\n",
        "\n",
        "```python\n",
        "# Data manipulation and splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Deep learning library\n",
        "import tensorflow as tf\n",
        "```\n",
        "\n",
        "## **Step 2: Loading the Data**\n",
        "\n",
        "We'll be using a dataset named `charity_data.csv` which contains relevant information about different charities. Let's load this data and take a quick look at the first few rows:\n",
        "\n",
        "```python\n",
        "# Import the dataset\n",
        "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "application_df.head()\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRnwtp4e2P_q"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Alphabet Soup's Dataset Information\n",
        "\n",
        "From Alphabet Soupâ€™s business team, we have obtained a comprehensive CSV dataset. This dataset encompasses data from over 34,000 organizations that have been beneficiaries of funding from Alphabet Soup throughout the years.\n",
        "\n",
        "## Columns Description:\n",
        "\n",
        "The dataset provides a range of columns, capturing metadata about each organization:\n",
        "\n",
        "- **EIN and NAME**: Identification columns.\n",
        "  \n",
        "- **APPLICATION_TYPE**: Describes the Alphabet Soup application type.\n",
        "\n",
        "- **AFFILIATION**: Represents the affiliated sector of the industry.\n",
        "\n",
        "- **CLASSIFICATION**: Refers to the government organization classification.\n",
        "\n",
        "- **USE_CASE**: Specifies the use case for which the funding is granted.\n",
        "\n",
        "- **ORGANIZATION**: Type of the organization.\n",
        "\n",
        "- **STATUS**: Indicates whether the organization is currently active or not.\n",
        "\n",
        "- **INCOME_AMT**: Categorizes organizations based on their income.\n",
        "\n",
        "- **SPECIAL_CONSIDERATIONS**: Notes if there are any special considerations for the application.\n",
        "\n",
        "- **ASK_AMT**: The amount of funding the organization requested.\n",
        "\n",
        "- **IS_SUCCESSFUL**: Determines whether the funding was utilized effectively.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBzuWzwS2P_r",
        "outputId": "522fbca7-a79b-4151-cb63-017117ed9802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 34299 entries, 0 to 34298\n",
            "Data columns (total 12 columns):\n",
            " #   Column                  Non-Null Count  Dtype \n",
            "---  ------                  --------------  ----- \n",
            " 0   EIN                     34299 non-null  int64 \n",
            " 1   NAME                    34299 non-null  object\n",
            " 2   APPLICATION_TYPE        34299 non-null  object\n",
            " 3   AFFILIATION             34299 non-null  object\n",
            " 4   CLASSIFICATION          34299 non-null  object\n",
            " 5   USE_CASE                34299 non-null  object\n",
            " 6   ORGANIZATION            34299 non-null  object\n",
            " 7   STATUS                  34299 non-null  int64 \n",
            " 8   INCOME_AMT              34299 non-null  object\n",
            " 9   SPECIAL_CONSIDERATIONS  34299 non-null  object\n",
            " 10  ASK_AMT                 34299 non-null  int64 \n",
            " 11  IS_SUCCESSFUL           34299 non-null  int64 \n",
            "dtypes: int64(4), object(8)\n",
            "memory usage: 3.1+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#  Import and read the charity_data.csv.\n",
        "import pandas as pd\n",
        "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
        "application_df.head()\n",
        "print(application_df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UR9R4fE2P_t"
      },
      "source": [
        "\n",
        "\n",
        "## Data Cleaning\n",
        "\n",
        "### Dropping Non-Beneficial Columns\n",
        "\n",
        "For our analysis, certain columns such as 'EIN' and 'NAME' are not beneficial. These columns serve as identification for the organizations but do not offer any substantive information for our machine learning model. Therefore, we'll drop these columns from our dataset:\n",
        "\n",
        "```python\n",
        "application_df.drop(columns=[\"EIN\", \"NAME\"], inplace=True)\n",
        "```\n",
        "\n",
        "**Note**: It's essential to include the `inplace=True` parameter if you wish to modify the `application_df` directly. Without it, the changes won't be saved to the `application_df` but will return a new DataFrame with the columns dropped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6w_BmF_2P_t",
        "outputId": "3f5cb077-d1b4-45f3-bf13-22a76e06da41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    18261\n",
              "0    16038\n",
              "Name: IS_SUCCESSFUL, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
        "application_df.drop(columns=[\"EIN\" , \"NAME\"] , inplace= True)\n",
        "\n",
        "#check data\n",
        "application_df.head()\n",
        "\n",
        "#check target: to see if there are any bug difrenceses in value count\n",
        "application_df[\"IS_SUCCESSFUL\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYwF8l232P_u"
      },
      "source": [
        "\n",
        "## Unique Value Analysis\n",
        "\n",
        "To understand the diversity of our dataset and to ascertain potential candidates for one-hot encoding (among other preprocessing steps), we first determine the number of unique values in each column:\n",
        "\n",
        "```python\n",
        "application_df.nunique()\n",
        "```\n",
        "\n",
        "By examining the unique count, we can get insights into the categorical nature of our data and decide on the next preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "application_df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFE2tUAr7F2W",
        "outputId": "be29bc99-19c0-42aa-b9f5-bb507865beb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "APPLICATION_TYPE          object\n",
              "AFFILIATION               object\n",
              "CLASSIFICATION            object\n",
              "USE_CASE                  object\n",
              "ORGANIZATION              object\n",
              "STATUS                     int64\n",
              "INCOME_AMT                object\n",
              "SPECIAL_CONSIDERATIONS    object\n",
              "ASK_AMT                    int64\n",
              "IS_SUCCESSFUL              int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPPtXa_t2P_u",
        "outputId": "2053be5e-e126-4fd5-d3bb-2d4ceafb24b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "APPLICATION_TYPE            17\n",
              "AFFILIATION                  6\n",
              "CLASSIFICATION              71\n",
              "USE_CASE                     5\n",
              "ORGANIZATION                 4\n",
              "STATUS                       2\n",
              "INCOME_AMT                   9\n",
              "SPECIAL_CONSIDERATIONS       2\n",
              "ASK_AMT                   8747\n",
              "IS_SUCCESSFUL                2\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Determine the number of unique values in each column.\n",
        "application_df_count = application_df.nunique()\n",
        "application_df_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD-H0gDU2P_v",
        "outputId": "273624bf-3030-449d-8ccd-b4107ef53043"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T3     27037\n",
              "T4      1542\n",
              "T6      1216\n",
              "T5      1173\n",
              "T19     1065\n",
              "T8       737\n",
              "T7       725\n",
              "T10      528\n",
              "T9       156\n",
              "T13       66\n",
              "T12       27\n",
              "T2        16\n",
              "T25        3\n",
              "T14        3\n",
              "T29        2\n",
              "T15        2\n",
              "T17        1\n",
              "Name: APPLICATION_TYPE, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look at APPLICATION_TYPE value counts for binning\n",
        "application_type_count =  application_df[\"APPLICATION_TYPE\"].value_counts()\n",
        "application_type_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1R8MlO2P_v"
      },
      "source": [
        "\n",
        "\n",
        "1. **Choosing a Cutoff**:\n",
        "    ```python\n",
        "    application_types_to_replace = application_type_count[8:].index\n",
        "    ```\n",
        "   Here, the code selects a cutoff based on the index. The `[8:]` slice means you're taking all application types from the 8th index onward. This assumes that `application_type_count` is already sorted in descending order by count. The code is effectively saying, \"Let's replace all application types from the 8th least frequent type and beyond.\"\n",
        "\n",
        "2. **Replacement**:\n",
        "    ```python\n",
        "    for app in application_types_to_replace:\n",
        "        application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "    ```\n",
        "    This loop replaces all the application types in the `application_types_to_replace` list with the label \"Other\" in the DataFrame `application_df`.\n",
        "\n",
        "3. **Checking**:\n",
        "    ```python\n",
        "    application_df['APPLICATION_TYPE'].value_counts()\n",
        "    ```\n",
        "    This line checks and displays the updated counts for each `APPLICATION_TYPE`, verifying that the replacement was successful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR2D4vPN2P_w"
      },
      "source": [
        "---\n",
        "\n",
        "### Explanation\n",
        "\n",
        "---\n",
        "\n",
        "## **Minimizing Application Types**\n",
        "\n",
        "In our dataset, there are multiple `APPLICATION_TYPE`s, some of which occur very infrequently. Having many infrequent categories can sometimes introduce noise into our machine learning models, making them less effective. By reducing the number of infrequent categories, we aim to create a more generalizable model.\n",
        "\n",
        "### **Procedure**:\n",
        "\n",
        "1. **Determine Cutoff**:\n",
        "   \n",
        "   We choose a cutoff to determine which application types are considered \"infrequent.\" For our current approach, we select all application types from the 8th least frequent type onward. These types will be replaced with a more general label: \"Other.\"\n",
        "\n",
        "    ```python\n",
        "    application_types_to_replace = application_type_count[8:].index\n",
        "    ```\n",
        "\n",
        "2. **Replacement**:\n",
        "   \n",
        "   Next, we iterate over the identified application types and replace them in our dataset.\n",
        "\n",
        "    ```python\n",
        "    for app in application_types_to_replace:\n",
        "        application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "    ```\n",
        "\n",
        "3. **Validation**:\n",
        "   \n",
        "   Finally, to ensure our changes have taken effect, we check the updated value counts for `APPLICATION_TYPE`.\n",
        "\n",
        "    ```python\n",
        "    application_df['APPLICATION_TYPE'].value_counts()\n",
        "    ```\n",
        "\n",
        "### **Rationale**:\n",
        "\n",
        "Minimizing the number of categories in our dataset can:\n",
        "\n",
        "- **Enhance Model Generalization**: By reducing sparse categories, we reduce the chance of overfitting to specific categories that don't have enough representation.\n",
        "  \n",
        "- **Streamline Encoding**: Later, when we encode these categories for modeling, fewer categories can lead to fewer columns, making our dataset more manageable and the model faster.\n",
        "\n",
        "- **Improve Interpretability**: Models become more interpretable with fewer categories as the focus shifts to more significant, broad categories rather than nuanced, infrequent ones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hregHut2P_w",
        "outputId": "f24df3ec-1648-42d5-9b81-048388b49d76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T3       27037\n",
              "T4        1542\n",
              "T6        1216\n",
              "T5        1173\n",
              "T19       1065\n",
              "T8         737\n",
              "T7         725\n",
              "T10        528\n",
              "Other      276\n",
              "Name: APPLICATION_TYPE, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose a cutoff value and create a list of application types to be replaced\n",
        "# use the variable name `application_types_to_replace`\n",
        "application_types_to_replace = application_type_count[8:].index\n",
        "\n",
        "#review video\n",
        "application_types_to_replace = list()\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in application_types_to_replace:\n",
        "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "\n",
        "# Check to make sure binning was successful\n",
        "application_df['APPLICATION_TYPE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nPm9jq92P_w",
        "outputId": "ad3c095d-c3c1-42ad-df32-807a9e5fa217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "Index: 71 entries, C1000 to C2150\n",
            "Series name: CLASSIFICATION\n",
            "Non-Null Count  Dtype\n",
            "--------------  -----\n",
            "71 non-null     int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 1.1+ KB\n"
          ]
        }
      ],
      "source": [
        "# Look at CLASSIFICATION value counts for binning\n",
        "classification_count = application_df[\"CLASSIFICATION\"].value_counts()\n",
        "classification_count.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classi"
      ],
      "metadata": {
        "id": "RB2PrfiK8z7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUQduTUx870V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsZCd-BB2P_w",
        "outputId": "1cfd211f-7851-4ff0-b2ad-519cb164c739"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "C1000    17326\n",
              "C2000     6074\n",
              "C1200     4837\n",
              "C3000     1918\n",
              "C2100     1883\n",
              "C7000      777\n",
              "C1700      287\n",
              "C4000      194\n",
              "C5000      116\n",
              "C1270      114\n",
              "C2700      104\n",
              "C2800       95\n",
              "C7100       75\n",
              "C1300       58\n",
              "C1280       50\n",
              "C1230       36\n",
              "C1400       34\n",
              "C7200       32\n",
              "C2300       32\n",
              "C1240       30\n",
              "C8000       20\n",
              "C7120       18\n",
              "C1500       16\n",
              "C1800       15\n",
              "C6000       15\n",
              "C1250       14\n",
              "C8200       11\n",
              "C1278       10\n",
              "C1238       10\n",
              "C1237        9\n",
              "C1235        9\n",
              "C7210        7\n",
              "C1720        6\n",
              "C2400        6\n",
              "C4100        6\n",
              "C1257        5\n",
              "C1600        5\n",
              "C0           3\n",
              "C1260        3\n",
              "C2710        3\n",
              "C1234        2\n",
              "C1246        2\n",
              "C1267        2\n",
              "C3200        2\n",
              "C1256        2\n",
              "Name: CLASSIFICATION, dtype: int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
        "#  YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace"
      ],
      "metadata": {
        "id": "0gXGButH9TGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PazBY-B32P_x",
        "outputId": "74274890-422c-4779-8276-778a6c96d2f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "C1000    17326\n",
              "C2000     6074\n",
              "C1200     4837\n",
              "Other     2261\n",
              "C3000     1918\n",
              "C2100     1883\n",
              "Name: CLASSIFICATION, dtype: int64"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Choose a cutoff value and create a list of classifications to be replaced\n",
        "# use the variable name `classifications_to_replace`\n",
        "replace\n",
        "# Replace in dataframe\n",
        "for cls in classifications_to_replace:\n",
        "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
        "\n",
        "# Check to make sure binning was successful\n",
        "application_df['CLASSIFICATION'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1soqfm0k2P_x"
      },
      "outputs": [],
      "source": [
        "# Convert categorical data to numeric with `pd.get_dummies`\n",
        "#  YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "4JhHMx_Q2P_x",
        "outputId": "72312f33-1aba-4488-db68-c1762d038a6f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a4b3d917b911>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplication_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IS_SUCCESSFUL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplication_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"IS_SUCCESSFUL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSHAPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the preprocessed data into a training and testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#  YOUR CODE GOES HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'SHAPE'"
          ]
        }
      ],
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = application_df[\"IS_SUCCESSFUL\"]\n",
        "X = application_df.drop(columns=\"IS_SUCCESSFUL\")\n",
        "X.SHAPE\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "#  YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yNViUGL2P_x"
      },
      "outputs": [],
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-vXXQAP2P_x"
      },
      "source": [
        "## Compile, Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EDgdxa02P_y",
        "outputId": "838aab8a-7154-4dd9-b699-cd03a898c91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From Z:\\Travis\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 80)                3520      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 30)                2430      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 5,981\n",
            "Trainable params: 5,981\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "#  YOUR CODE GOES HERE\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "#  YOUR CODE GOES HERE\n",
        "\n",
        "# Second hidden layer\n",
        "#  YOUR CODE GOES HERE\n",
        "\n",
        "# Output layer\n",
        "#  YOUR CODE GOES HERE\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw4VfwAk2P_y",
        "outputId": "5ecd8430-cad3-4e01-84cd-5f02e45fdbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From Z:\\Travis\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "#  YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXqvqJQS2P_y",
        "outputId": "9b3e9361-a582-4445-c263-b6b4e544d691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25724/25724 [==============================] - 2s 80us/sample - loss: 0.5700 - acc: 0.7234\n",
            "Epoch 2/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5556 - acc: 0.7294\n",
            "Epoch 3/100\n",
            "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5529 - acc: 0.7304\n",
            "Epoch 4/100\n",
            "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5514 - acc: 0.7318\n",
            "Epoch 5/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5495 - acc: 0.7322\n",
            "Epoch 6/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5488 - acc: 0.7330\n",
            "Epoch 7/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5482 - acc: 0.7335\n",
            "Epoch 8/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5475 - acc: 0.7326\n",
            "Epoch 9/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5468 - acc: 0.73491s - lo\n",
            "Epoch 10/100\n",
            "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5465 - acc: 0.7350\n",
            "Epoch 11/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5459 - acc: 0.7345\n",
            "Epoch 12/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5456 - acc: 0.7362\n",
            "Epoch 13/100\n",
            "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5458 - acc: 0.7339\n",
            "Epoch 14/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5450 - acc: 0.7361\n",
            "Epoch 15/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5447 - acc: 0.7353\n",
            "Epoch 16/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5444 - acc: 0.7356\n",
            "Epoch 17/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5440 - acc: 0.7358\n",
            "Epoch 18/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5442 - acc: 0.7367\n",
            "Epoch 19/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5436 - acc: 0.7360\n",
            "Epoch 20/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5435 - acc: 0.7365\n",
            "Epoch 21/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5426 - acc: 0.7377\n",
            "Epoch 22/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5429 - acc: 0.7367\n",
            "Epoch 23/100\n",
            "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5428 - acc: 0.7382\n",
            "Epoch 24/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5425 - acc: 0.7376\n",
            "Epoch 25/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5421 - acc: 0.7379\n",
            "Epoch 26/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5419 - acc: 0.7382\n",
            "Epoch 27/100\n",
            "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5415 - acc: 0.7384\n",
            "Epoch 28/100\n",
            "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5418 - acc: 0.7381\n",
            "Epoch 29/100\n",
            "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5415 - acc: 0.7381\n",
            "Epoch 30/100\n",
            "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5410 - acc: 0.7382\n",
            "Epoch 31/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5414 - acc: 0.7381\n",
            "Epoch 32/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5407 - acc: 0.7388\n",
            "Epoch 33/100\n",
            "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5412 - acc: 0.7374\n",
            "Epoch 34/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5405 - acc: 0.7383\n",
            "Epoch 35/100\n",
            "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5405 - acc: 0.7388\n",
            "Epoch 36/100\n",
            "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5405 - acc: 0.7384\n",
            "Epoch 37/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5403 - acc: 0.7379\n",
            "Epoch 38/100\n",
            "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5401 - acc: 0.7392\n",
            "Epoch 39/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5405 - acc: 0.7387\n",
            "Epoch 40/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5399 - acc: 0.7390\n",
            "Epoch 41/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5395 - acc: 0.7394\n",
            "Epoch 42/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5399 - acc: 0.7391\n",
            "Epoch 43/100\n",
            "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5396 - acc: 0.7389\n",
            "Epoch 44/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5398 - acc: 0.7397\n",
            "Epoch 45/100\n",
            "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5395 - acc: 0.7400\n",
            "Epoch 46/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5389 - acc: 0.7391\n",
            "Epoch 47/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5391 - acc: 0.7393\n",
            "Epoch 48/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5390 - acc: 0.7383\n",
            "Epoch 49/100\n",
            "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5391 - acc: 0.7390\n",
            "Epoch 50/100\n",
            "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5387 - acc: 0.7402\n",
            "Epoch 51/100\n",
            "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5388 - acc: 0.7393\n",
            "Epoch 52/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5389 - acc: 0.7393\n",
            "Epoch 53/100\n",
            "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5387 - acc: 0.7394\n",
            "Epoch 54/100\n",
            "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5385 - acc: 0.7395\n",
            "Epoch 55/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5381 - acc: 0.7394\n",
            "Epoch 56/100\n",
            "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5384 - acc: 0.7396\n",
            "Epoch 57/100\n",
            "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5384 - acc: 0.7392\n",
            "Epoch 58/100\n",
            "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5382 - acc: 0.7398\n",
            "Epoch 59/100\n",
            "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5382 - acc: 0.7390\n",
            "Epoch 60/100\n",
            "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5383 - acc: 0.7395\n",
            "Epoch 61/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5380 - acc: 0.7393\n",
            "Epoch 62/100\n",
            "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5386 - acc: 0.7395\n",
            "Epoch 63/100\n",
            "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5379 - acc: 0.7397\n",
            "Epoch 64/100\n",
            "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5383 - acc: 0.7414\n",
            "Epoch 65/100\n",
            "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5376 - acc: 0.74031s \n",
            "Epoch 66/100\n",
            "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5378 - acc: 0.7407\n",
            "Epoch 67/100\n",
            "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5378 - acc: 0.7402\n",
            "Epoch 68/100\n",
            "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5374 - acc: 0.7403\n",
            "Epoch 69/100\n",
            "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5376 - acc: 0.7400\n",
            "Epoch 70/100\n",
            "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5377 - acc: 0.7404\n",
            "Epoch 71/100\n",
            "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5377 - acc: 0.7402\n",
            "Epoch 72/100\n",
            "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5374 - acc: 0.7403\n",
            "Epoch 73/100\n",
            "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5375 - acc: 0.7400\n",
            "Epoch 74/100\n",
            "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5373 - acc: 0.7407\n",
            "Epoch 75/100\n",
            "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5371 - acc: 0.7402\n",
            "Epoch 76/100\n",
            "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5372 - acc: 0.7404\n",
            "Epoch 77/100\n",
            "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5372 - acc: 0.7399\n",
            "Epoch 78/100\n",
            "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5378 - acc: 0.7409\n",
            "Epoch 79/100\n",
            "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5370 - acc: 0.7410\n",
            "Epoch 80/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5371 - acc: 0.7402\n",
            "Epoch 81/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5368 - acc: 0.7414\n",
            "Epoch 82/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5366 - acc: 0.7413\n",
            "Epoch 83/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5368 - acc: 0.7405\n",
            "Epoch 84/100\n",
            "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5367 - acc: 0.7407\n",
            "Epoch 85/100\n",
            "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5368 - acc: 0.7404\n",
            "Epoch 86/100\n",
            "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5365 - acc: 0.7408\n",
            "Epoch 87/100\n",
            "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5371 - acc: 0.7407\n",
            "Epoch 88/100\n",
            "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5367 - acc: 0.7410\n",
            "Epoch 89/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5366 - acc: 0.7409\n",
            "Epoch 90/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5365 - acc: 0.7409\n",
            "Epoch 91/100\n",
            "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5360 - acc: 0.7414\n",
            "Epoch 92/100\n",
            "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5370 - acc: 0.7406\n",
            "Epoch 93/100\n",
            "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5365 - acc: 0.7411\n",
            "Epoch 94/100\n",
            "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5366 - acc: 0.7409\n",
            "Epoch 95/100\n",
            "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5362 - acc: 0.7411\n",
            "Epoch 96/100\n",
            "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5361 - acc: 0.7404\n",
            "Epoch 97/100\n",
            "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5361 - acc: 0.74130s - loss: 0.5380 - \n",
            "Epoch 98/100\n",
            "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5365 - acc: 0.7407\n",
            "Epoch 99/100\n",
            "25724/25724 [==============================] - 2s 80us/sample - loss: 0.5360 - acc: 0.7396\n",
            "Epoch 100/100\n",
            "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5360 - acc: 0.7416\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "#  YOUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5fmFwh22P_y",
        "outputId": "91979626-82d8-40f6-a9f7-f41dc31c0af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8575/8575 - 0s - loss: 0.5578 - acc: 0.7263\n",
            "Loss: 0.557812534073699, Accuracy: 0.7262973785400391\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G9V__Vq2P_y"
      },
      "outputs": [],
      "source": [
        "# Export our model to HDF5 file\n",
        "#  YOUR CODE GOES HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}